import streamlit as st
import os
import shutil

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS


# --- è¨­å®šé é¢ ---
st.set_page_config(page_title="RAG çŸ¥è­˜åº«ç³»çµ± (å«åˆ†æ•¸é¡¯ç¤º)", layout="wide", page_icon="ğŸ”¢")
st.title("ğŸ”¢ RAG ç³»çµ± (é¡¯ç¤ºç›¸ä¼¼åº¦åˆ†æ•¸)")

# --- æ ¸å¿ƒè·¯å¾‘è¨­å®š ---
DB_PATH = "faiss_db_output"     # å‘é‡è³‡æ–™åº«å„²å­˜ä½ç½®
DOCS_DIR = "source_data"        # åŸå§‹æ–‡ä»¶å„²å­˜ä½ç½®

# ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨
os.makedirs(DOCS_DIR, exist_ok=True)

# --- åˆå§‹åŒ– Session State ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- è¼‰å…¥æ¨¡å‹ (å¿«å–) ---
@st.cache_resource
def load_embedding_model():
    # ä½¿ç”¨ HuggingFace çš„ Embedding æ¨¡å‹
    return HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-small")

# --- åŠŸèƒ½å‡½å¼ ---

def save_uploaded_file(uploaded_file):
    """å°‡ä¸Šå‚³çš„æª”æ¡ˆå„²å­˜åˆ° DOCS_DIR"""
    file_path = os.path.join(DOCS_DIR, uploaded_file.name)
    with open(file_path, "wb") as f:
        f.write(uploaded_file.getbuffer())
    return file_path

def generate_sample_data():
    """ç”Ÿæˆç¯„ä¾‹æ–‡ä»¶ä¸¦å­˜å…¥ DOCS_DIR"""
    samples = {
        "AIç ”ç©¶ç¤¾_ä»‹ç´¹.txt": "AIç ”ç©¶ç¤¾æˆç«‹æ–¼2023å¹´ï¼Œç¤¾é•·æ˜¯ç‹å°æ˜ã€‚ç¤¾èª²æ™‚é–“ç‚ºæ¯é€±äº”æ™šä¸Š7é»ï¼Œåœ°é»åœ¨è³‡è¨Šå¤§æ¨“305æ•™å®¤ã€‚æˆ‘å€‘çš„å®—æ—¨åœ¨æ–¼æ¨å»£ç”Ÿæˆå¼AIæŠ€è¡“ã€‚",
        "ç™»å±±ç¤¾_æ´»å‹•è¦ç« .txt": "ç™»å±±ç¤¾å®‰å…¨è¦ç« ï¼š1. åƒåŠ ç™¾å²³è¡Œç¨‹éœ€å…·å‚™åŸºç¤é«”èƒ½è­‰æ˜ã€‚ 2. è£å‚™æª¢æŸ¥æœªé€šéè€…ä¸å¾—ä¸Šå±±ã€‚ 3. é‡åˆ°é¢±é¢¨è­¦å ±ä¸€å¾‹å–æ¶ˆè¡Œç¨‹ã€‚è²»ç”¨éƒ¨åˆ†ï¼šç¤¾å“¡ç”±ç¤¾è²»è£œåŠ©20%ï¼Œéç¤¾å“¡å…¨é¡è‡ªè²»ã€‚",
        "åœ–æ›¸é¤¨_å€Ÿé–±è¦å‰‡.txt": "åœ–æ›¸é¤¨é–‹æ”¾æ™‚é–“ç‚ºé€±ä¸€è‡³é€±äº” 08:00-22:00ã€‚å¤§å­¸éƒ¨å­¸ç”Ÿå¯å€Ÿé–±10æœ¬æ›¸ï¼Œå€ŸæœŸ30å¤©ã€‚é€¾æœŸç½°æ¬¾æ¯æ—¥æ¯æœ¬5å…ƒã€‚éºå¤±åœ–æ›¸éœ€è³ å„ŸåŸåƒ¹ä¹‹1.5å€ã€‚"
    }
    
    for filename, content in samples.items():
        path = os.path.join(DOCS_DIR, filename)
        with open(path, "w", encoding="utf-8") as f:
            f.write(content)
    
    return list(samples.keys())

def build_vector_db():
    """è®€å– DOCS_DIR ä¸­çš„æ‰€æœ‰æª”æ¡ˆä¸¦å»ºç«‹å‘é‡åº«"""
    embedding_model = load_embedding_model()
    documents = []
    
    # æƒæ DOCS_DIR è³‡æ–™å¤¾
    files = [f for f in os.listdir(DOCS_DIR) if f.endswith(('.txt', '.pdf'))]
    
    if not files:
        return False, "è³‡æ–™å¤¾ä¸­æ²’æœ‰æ–‡ä»¶ï¼Œè«‹å…ˆä¸Šå‚³æˆ–ç”Ÿæˆè³‡æ–™ã€‚"

    progress_bar = st.progress(0, text="æ­£åœ¨è®€å–æª”æ¡ˆ...")
    
    for i, file in enumerate(files):
        file_path = os.path.join(DOCS_DIR, file)
        try:
            if file.endswith(".txt"):
                loader = TextLoader(file_path, encoding="utf-8")
            elif file.endswith(".pdf"):
                loader = PyPDFLoader(file_path)
            else:
                continue
            
            # è¼‰å…¥ä¸¦æ¨™è¨˜ä¾†æº
            docs = loader.load()
            for doc in docs:
                doc.metadata["source"] = file # ç¢ºä¿ metadata æœ‰æª”å
            documents.extend(docs)
            
        except Exception as e:
            st.error(f"è®€å– {file} å¤±æ•—: {e}")
        
        progress_bar.progress((i + 1) / len(files), text=f"å·²è®€å–: {file}")

    if not documents:
        return False, "æ²’æœ‰æœ‰æ•ˆå…§å®¹å¯å»ºç«‹ç´¢å¼•ã€‚"

    # åˆ‡åˆ†èˆ‡å‘é‡åŒ–
    progress_bar.progress(0.8, text="æ­£åœ¨åˆ‡åˆ†æ–‡æœ¬èˆ‡å»ºç«‹ç´¢å¼•...")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
    split_docs = text_splitter.split_documents(documents)
    
    vectorstore = FAISS.from_documents(split_docs, embedding_model)
    vectorstore.save_local(DB_PATH)
    
    progress_bar.progress(1.0, text="å®Œæˆï¼")
    return True, f"æˆåŠŸå»ºç«‹è³‡æ–™åº«ï¼å…±åŒ…å« {len(files)} ä»½æ–‡ä»¶ï¼Œåˆ‡åˆ†ç‚º {len(split_docs)} å€‹ç‰‡æ®µã€‚"

def query_rag(query_text):
    """æŸ¥è©¢å‘é‡è³‡æ–™åº«ä¸¦å›å‚³åˆ†æ•¸"""
    if not os.path.exists(DB_PATH):
        return "âš ï¸ è«‹å…ˆå»ºç«‹è³‡æ–™åº« (è«‹è‡³å·¦å´ 'å»ºç«‹çŸ¥è­˜åº«' åˆ†é )"
    
    embedding_model = load_embedding_model()
    vectorstore = FAISS.load_local(DB_PATH, embedding_model, allow_dangerous_deserialization=True)
    
    # ä¿®æ”¹é‡é»ï¼šä½¿ç”¨ similarity_search_with_score
    # k=10 è¨­å¤§ä¸€é»ï¼Œä»¥ä¾¿ç›¡å¯èƒ½é¡¯ç¤ºæ‰€æœ‰ç›¸é—œæ–‡ä»¶ (é‡å°å°è³‡æ–™é›†)
    results_with_score = vectorstore.similarity_search_with_score(query_text, k=10)
    
    response = f"ğŸ” **æŸ¥è©¢å…§å®¹**ï¼š{query_text}\n\n"
    response += "ğŸ“Š **æª¢ç´¢çµæœ (æŒ‰è·é›¢åˆ†æ•¸æ’åºï¼Œè¶Šä½ä»£è¡¨è¶Šç›¸ä¼¼)**ï¼š\n\n"
    
    for i, (doc, score) in enumerate(results_with_score):
        # å˜—è©¦å¾ metadata ç²å–æª”å
        source = doc.metadata.get('source', 'æœªçŸ¥ä¾†æº')
        source_name = os.path.basename(source)
        
        # æ ¼å¼åŒ–è¼¸å‡º
        response += f"**#{i+1} ä¾†æº**: `{source_name}`\n"
        response += f"ğŸ”´ **è·é›¢åˆ†æ•¸ (Distance Score)**: `{score:.5f}`\n" 
        response += f"ğŸ“„ **å…§å®¹ç‰‡æ®µ**: {doc.page_content}\n\n---\n"
        
    return response

# --- ä»‹é¢ä½ˆå±€ ---

tab1, tab2 = st.tabs(["ğŸ“‚ ç®¡ç†èˆ‡ç€è¦½æ–‡ä»¶", "ğŸ’¬ AI åŠ©æ‰‹å°è©±"])

with tab1:
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("1. æ–°å¢è³‡æ–™")
        st.info("ä¸Šå‚³æˆ–ç”Ÿæˆæª”æ¡ˆå¾Œï¼Œæª”æ¡ˆæœƒå­˜å…¥ `source_data` è³‡æ–™å¤¾ã€‚")
        
        # ç”Ÿæˆç¯„ä¾‹æŒ‰éˆ•
        if st.button("âœ¨ ç”Ÿæˆæ¸¬è©¦ç”¨æ–‡ä»¶"):
            files = generate_sample_data()
            st.success(f"å·²ç”Ÿæˆ {len(files)} ä»½æ–‡ä»¶ã€‚")
            st.rerun() 

        # ä¸Šå‚³æŒ‰éˆ•
        uploaded_files = st.file_uploader("ä¸Šå‚³æ–°æ–‡ä»¶ (.txt, .pdf)", accept_multiple_files=True)
        if uploaded_files:
            for u_file in uploaded_files:
                save_uploaded_file(u_file)
            st.success(f"å·²å„²å­˜ {len(uploaded_files)} ä»½æ–°æ–‡ä»¶ã€‚")
            st.rerun()

        st.divider()
        
        st.subheader("2. å»ºç«‹/æ›´æ–° è³‡æ–™åº«")
        if st.button("ğŸš€ é‡å»º RAG ç´¢å¼•"):
            with st.spinner("æ­£åœ¨è™•ç†..."):
                success, msg = build_vector_db()
                if success:
                    st.success(msg)
                else:
                    st.error(msg)

    with col2:
        st.subheader("ğŸ“š æª¢è¦–ç›®å‰æ–‡ä»¶")
        st.caption(f"è³‡æ–™å¤¾è·¯å¾‘: {DOCS_DIR}")
        
        existing_files = os.listdir(DOCS_DIR)
        
        if not existing_files:
            st.write("ç›®å‰æ²’æœ‰ä»»ä½•æ–‡ä»¶ã€‚")
        else:
            for f in existing_files:
                file_path = os.path.join(DOCS_DIR, f)
                with st.expander(f"ğŸ“„ {f}"):
                    if st.button("åˆªé™¤", key=f"del_{f}"):
                        os.remove(file_path)
                        st.rerun()
                    
                    if f.endswith(".txt"):
                        with open(file_path, "r", encoding="utf-8") as _f:
                            st.text(_f.read())
                    else:
                        st.write("PDF æª”æ¡ˆåƒ…æ”¯æ´é è¦½æª”åèˆ‡è·¯å¾‘ã€‚")

with tab2:
    st.header("èˆ‡ä½ çš„æ–‡ä»¶å°è©±")
    
    if not os.path.exists(DB_PATH):
        st.warning("âš ï¸ å°šæœªåµæ¸¬åˆ°å‘é‡è³‡æ–™åº«ï¼Œè«‹å…ˆè‡³ã€Œç®¡ç†èˆ‡ç€è¦½æ–‡ä»¶ã€åˆ†é å»ºç«‹ç´¢å¼•ã€‚")

    # é¡¯ç¤ºæ­·å²è¨Šæ¯
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # è™•ç†è¼¸å…¥
    if prompt := st.chat_input("è«‹è¼¸å…¥å•é¡Œ..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("æœå°‹ä¸¦è¨ˆç®—åˆ†æ•¸ä¸­..."):
                response = query_rag(prompt)
                st.markdown(response)
        
        st.session_state.messages.append({"role": "assistant", "content": response})